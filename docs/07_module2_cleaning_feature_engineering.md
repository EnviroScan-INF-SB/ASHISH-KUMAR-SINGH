# Module 2: Data Cleaning and Feature Engineering

## Purpose
Implement data cleaning, missing value handling, unit standardization, normalization, spatial proximity feature calculation, temporal feature derivation, and combine datasets into a unified DataFrame.

## Script
- `scripts/clean_and_feature_engineer.py`

## What it does
- Removes duplicates and invalid records.
- Handles missing values by interpolation (per location) and median imputation.
- Standardizes timestamps and coordinate fields.
- Pivots pollutant measurements into columns and normalizes pollutant/weather values using MinMax scaling.
- Derives temporal features: hour, day of week, month, season.
- Attempts to merge geographic proximity features from `data/geographic_features_summary.csv`.
- Saves outputs:
  - `data/processed_data.csv`
  - `data/processed_data_normalized.csv`
  - `data/feature_list.json`

## How to run
From project root:
```powershell
python scripts/clean_and_feature_engineer.py
```

## Notes
- The script expects outputs from Module 1 (`openaq_air_quality_data.csv`, `weather_data_enhanced.csv`).
- Spatial proximity relies on the geographic summary generated by Module 1.

## Verification
- Check `data/processed_data.csv` for pollutant columns (`pm25`, `pm10`, `no2`, `co`, `so2`, `o3`), normalized columns (e.g., `pm25_norm`), and temporal features (`hour`, `dayofweek`, `season`).